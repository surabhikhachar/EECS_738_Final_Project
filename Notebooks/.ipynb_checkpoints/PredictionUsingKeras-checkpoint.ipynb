{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "This notebook will be used to implement a neural network using Keras. The goal is to implement a neural network to predict the outcome of an NCAA basketball game. First we will perform feature engineering to ensure the data is formatted such that it will fit in the neural network. First we will drop all non required fields. Then we use a label encoder to encode the winner attribute so that it is a 0 or 1. This is a requirement for the output node in the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>away_assist_percentage</th>\n",
       "      <th>away_assists</th>\n",
       "      <th>away_block_percentage</th>\n",
       "      <th>away_blocks</th>\n",
       "      <th>away_defensive_rating</th>\n",
       "      <th>away_defensive_rebound_percentage</th>\n",
       "      <th>away_defensive_rebounds</th>\n",
       "      <th>away_effective_field_goal_percentage</th>\n",
       "      <th>away_field_goal_attempts</th>\n",
       "      <th>away_field_goal_percentage</th>\n",
       "      <th>...</th>\n",
       "      <th>home_two_point_field_goals</th>\n",
       "      <th>home_win_percentage</th>\n",
       "      <th>home_wins</th>\n",
       "      <th>location</th>\n",
       "      <th>losing_abbr</th>\n",
       "      <th>losing_name</th>\n",
       "      <th>pace</th>\n",
       "      <th>winner</th>\n",
       "      <th>winning_abbr</th>\n",
       "      <th>winning_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.3</td>\n",
       "      <td>5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1</td>\n",
       "      <td>131.0</td>\n",
       "      <td>52.4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.413</td>\n",
       "      <td>52</td>\n",
       "      <td>0.365</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>0.900</td>\n",
       "      <td>9</td>\n",
       "      <td>Moody Coliseum , Abilene, Texas</td>\n",
       "      <td>Schreiner\\n\\t\\t\\t</td>\n",
       "      <td>Schreiner\\n\\t\\t\\t</td>\n",
       "      <td>71.0</td>\n",
       "      <td>Home</td>\n",
       "      <td>ABILENE-CHRISTIAN</td>\n",
       "      <td>Abilene Christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.7</td>\n",
       "      <td>4</td>\n",
       "      <td>6.1</td>\n",
       "      <td>3</td>\n",
       "      <td>120.5</td>\n",
       "      <td>45.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.318</td>\n",
       "      <td>55</td>\n",
       "      <td>0.273</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.846</td>\n",
       "      <td>11</td>\n",
       "      <td>Moody Coliseum , Abilene, Texas</td>\n",
       "      <td>McMurry\\n\\t\\t\\t</td>\n",
       "      <td>McMurry\\n\\t\\t\\t</td>\n",
       "      <td>73.2</td>\n",
       "      <td>Home</td>\n",
       "      <td>ABILENE-CHRISTIAN</td>\n",
       "      <td>Abilene Christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.3</td>\n",
       "      <td>6</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2</td>\n",
       "      <td>120.3</td>\n",
       "      <td>87.5</td>\n",
       "      <td>21</td>\n",
       "      <td>0.522</td>\n",
       "      <td>45</td>\n",
       "      <td>0.489</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.889</td>\n",
       "      <td>8</td>\n",
       "      <td>Moody Coliseum , Abilene, Texas</td>\n",
       "      <td>CAMPBELL</td>\n",
       "      <td>Campbell</td>\n",
       "      <td>69.3</td>\n",
       "      <td>Home</td>\n",
       "      <td>ABILENE-CHRISTIAN</td>\n",
       "      <td>Abilene Christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.3</td>\n",
       "      <td>6</td>\n",
       "      <td>10.3</td>\n",
       "      <td>3</td>\n",
       "      <td>110.8</td>\n",
       "      <td>71.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.439</td>\n",
       "      <td>57</td>\n",
       "      <td>0.386</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.800</td>\n",
       "      <td>24</td>\n",
       "      <td>Moody Coliseum , Abilene, Texas</td>\n",
       "      <td>STEPHEN-F-AUSTIN</td>\n",
       "      <td>Stephen F. Austin</td>\n",
       "      <td>65.1</td>\n",
       "      <td>Home</td>\n",
       "      <td>ABILENE-CHRISTIAN</td>\n",
       "      <td>Abilene Christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>142.7</td>\n",
       "      <td>50.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.418</td>\n",
       "      <td>49</td>\n",
       "      <td>0.367</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>Moody Coliseum , Abilene, Texas</td>\n",
       "      <td>Arlington Baptist\\n\\t\\t\\t</td>\n",
       "      <td>Arlington Baptist\\n\\t\\t\\t</td>\n",
       "      <td>74.8</td>\n",
       "      <td>Home</td>\n",
       "      <td>ABILENE-CHRISTIAN</td>\n",
       "      <td>Abilene Christian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   away_assist_percentage  away_assists  away_block_percentage  away_blocks  \\\n",
       "0                    26.3             5                    2.9            1   \n",
       "1                    26.7             4                    6.1            3   \n",
       "2                    27.3             6                    5.9            2   \n",
       "3                    27.3             6                   10.3            3   \n",
       "4                    27.8             5                    0.0            0   \n",
       "\n",
       "   away_defensive_rating  away_defensive_rebound_percentage  \\\n",
       "0                  131.0                               52.4   \n",
       "1                  120.5                               45.0   \n",
       "2                  120.3                               87.5   \n",
       "3                  110.8                               71.0   \n",
       "4                  142.7                               50.0   \n",
       "\n",
       "   away_defensive_rebounds  away_effective_field_goal_percentage  \\\n",
       "0                       11                                 0.413   \n",
       "1                       18                                 0.318   \n",
       "2                       21                                 0.522   \n",
       "3                       22                                 0.439   \n",
       "4                       13                                 0.418   \n",
       "\n",
       "   away_field_goal_attempts  away_field_goal_percentage  ...  \\\n",
       "0                        52                       0.365  ...   \n",
       "1                        55                       0.273  ...   \n",
       "2                        45                       0.489  ...   \n",
       "3                        57                       0.386  ...   \n",
       "4                        49                       0.367  ...   \n",
       "\n",
       "   home_two_point_field_goals  home_win_percentage  home_wins  \\\n",
       "0                          21                0.900          9   \n",
       "1                          27                0.846         11   \n",
       "2                          20                0.889          8   \n",
       "3                          16                0.800         24   \n",
       "4                          30                1.000          1   \n",
       "\n",
       "                          location                losing_abbr  \\\n",
       "0  Moody Coliseum , Abilene, Texas          Schreiner\\n\\t\\t\\t   \n",
       "1  Moody Coliseum , Abilene, Texas            McMurry\\n\\t\\t\\t   \n",
       "2  Moody Coliseum , Abilene, Texas                   CAMPBELL   \n",
       "3  Moody Coliseum , Abilene, Texas           STEPHEN-F-AUSTIN   \n",
       "4  Moody Coliseum , Abilene, Texas  Arlington Baptist\\n\\t\\t\\t   \n",
       "\n",
       "                 losing_name  pace  winner       winning_abbr  \\\n",
       "0          Schreiner\\n\\t\\t\\t  71.0    Home  ABILENE-CHRISTIAN   \n",
       "1            McMurry\\n\\t\\t\\t  73.2    Home  ABILENE-CHRISTIAN   \n",
       "2                   Campbell  69.3    Home  ABILENE-CHRISTIAN   \n",
       "3          Stephen F. Austin  65.1    Home  ABILENE-CHRISTIAN   \n",
       "4  Arlington Baptist\\n\\t\\t\\t  74.8    Home  ABILENE-CHRISTIAN   \n",
       "\n",
       "        winning_name  \n",
       "0  Abilene Christian  \n",
       "1  Abilene Christian  \n",
       "2  Abilene Christian  \n",
       "3  Abilene Christian  \n",
       "4  Abilene Christian  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df_NCAA = pd.read_csv('../data/data-full-good.csv')\n",
    "df_NCAA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NCAA = df_NCAA.drop(['winning_name', 'winning_abbr', 'losing_name', 'losing_abbr', 'location'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>away_assist_percentage</th>\n",
       "      <th>away_assists</th>\n",
       "      <th>away_block_percentage</th>\n",
       "      <th>away_blocks</th>\n",
       "      <th>away_defensive_rating</th>\n",
       "      <th>away_defensive_rebound_percentage</th>\n",
       "      <th>away_defensive_rebounds</th>\n",
       "      <th>away_effective_field_goal_percentage</th>\n",
       "      <th>away_field_goal_attempts</th>\n",
       "      <th>away_field_goal_percentage</th>\n",
       "      <th>...</th>\n",
       "      <th>home_true_shooting_percentage</th>\n",
       "      <th>home_turnover_percentage</th>\n",
       "      <th>home_turnovers</th>\n",
       "      <th>home_two_point_field_goal_attempts</th>\n",
       "      <th>home_two_point_field_goal_percentage</th>\n",
       "      <th>home_two_point_field_goals</th>\n",
       "      <th>home_win_percentage</th>\n",
       "      <th>home_wins</th>\n",
       "      <th>pace</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.3</td>\n",
       "      <td>5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1</td>\n",
       "      <td>131.0</td>\n",
       "      <td>52.4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.413</td>\n",
       "      <td>52</td>\n",
       "      <td>0.365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700</td>\n",
       "      <td>20.6</td>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>0.618</td>\n",
       "      <td>21</td>\n",
       "      <td>0.900</td>\n",
       "      <td>9</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.7</td>\n",
       "      <td>4</td>\n",
       "      <td>6.1</td>\n",
       "      <td>3</td>\n",
       "      <td>120.5</td>\n",
       "      <td>45.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.318</td>\n",
       "      <td>55</td>\n",
       "      <td>0.273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539</td>\n",
       "      <td>14.7</td>\n",
       "      <td>14</td>\n",
       "      <td>49</td>\n",
       "      <td>0.551</td>\n",
       "      <td>27</td>\n",
       "      <td>0.846</td>\n",
       "      <td>11</td>\n",
       "      <td>73.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.3</td>\n",
       "      <td>6</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2</td>\n",
       "      <td>120.3</td>\n",
       "      <td>87.5</td>\n",
       "      <td>21</td>\n",
       "      <td>0.522</td>\n",
       "      <td>45</td>\n",
       "      <td>0.489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688</td>\n",
       "      <td>14.4</td>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "      <td>0.588</td>\n",
       "      <td>20</td>\n",
       "      <td>0.889</td>\n",
       "      <td>8</td>\n",
       "      <td>69.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.3</td>\n",
       "      <td>6</td>\n",
       "      <td>10.3</td>\n",
       "      <td>3</td>\n",
       "      <td>110.8</td>\n",
       "      <td>71.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.439</td>\n",
       "      <td>57</td>\n",
       "      <td>0.386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546</td>\n",
       "      <td>10.9</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>0.552</td>\n",
       "      <td>16</td>\n",
       "      <td>0.800</td>\n",
       "      <td>24</td>\n",
       "      <td>65.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>142.7</td>\n",
       "      <td>50.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.418</td>\n",
       "      <td>49</td>\n",
       "      <td>0.367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704</td>\n",
       "      <td>12.8</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>0.698</td>\n",
       "      <td>30</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>74.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   away_assist_percentage  away_assists  away_block_percentage  away_blocks  \\\n",
       "0                    26.3             5                    2.9            1   \n",
       "1                    26.7             4                    6.1            3   \n",
       "2                    27.3             6                    5.9            2   \n",
       "3                    27.3             6                   10.3            3   \n",
       "4                    27.8             5                    0.0            0   \n",
       "\n",
       "   away_defensive_rating  away_defensive_rebound_percentage  \\\n",
       "0                  131.0                               52.4   \n",
       "1                  120.5                               45.0   \n",
       "2                  120.3                               87.5   \n",
       "3                  110.8                               71.0   \n",
       "4                  142.7                               50.0   \n",
       "\n",
       "   away_defensive_rebounds  away_effective_field_goal_percentage  \\\n",
       "0                       11                                 0.413   \n",
       "1                       18                                 0.318   \n",
       "2                       21                                 0.522   \n",
       "3                       22                                 0.439   \n",
       "4                       13                                 0.418   \n",
       "\n",
       "   away_field_goal_attempts  away_field_goal_percentage  ...  \\\n",
       "0                        52                       0.365  ...   \n",
       "1                        55                       0.273  ...   \n",
       "2                        45                       0.489  ...   \n",
       "3                        57                       0.386  ...   \n",
       "4                        49                       0.367  ...   \n",
       "\n",
       "   home_true_shooting_percentage  home_turnover_percentage  home_turnovers  \\\n",
       "0                          0.700                      20.6              17   \n",
       "1                          0.539                      14.7              14   \n",
       "2                          0.688                      14.4              10   \n",
       "3                          0.546                      10.9               8   \n",
       "4                          0.704                      12.8              11   \n",
       "\n",
       "   home_two_point_field_goal_attempts  home_two_point_field_goal_percentage  \\\n",
       "0                                  34                                 0.618   \n",
       "1                                  49                                 0.551   \n",
       "2                                  34                                 0.588   \n",
       "3                                  29                                 0.552   \n",
       "4                                  43                                 0.698   \n",
       "\n",
       "   home_two_point_field_goals  home_win_percentage  home_wins  pace  winner  \n",
       "0                          21                0.900          9  71.0       1  \n",
       "1                          27                0.846         11  73.2       1  \n",
       "2                          20                0.889          8  69.3       1  \n",
       "3                          16                0.800         24  65.1       1  \n",
       "4                          30                1.000          1  74.8       1  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(df_NCAA['winner'])\n",
    "df_NCAA['winner'] = le.transform(df_NCAA['winner'])\n",
    "df_NCAA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features we will use to train the neural network has been taken from the feature selection notebook. Please refer to that notebook for a further breakdown of how the following features have been selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X = df_NCAA[['away_defensive_rating', 'away_field_goals' ,'away_free_throws', 'away_offensive_rating', 'away_points', 'away_three_point_field_goals', 'away_wins', 'home_defensive_rating', 'home_field_goals', 'home_free_throws', 'home_offensive_rating', 'home_points', 'home_three_point_field_goals', 'home_wins']]\n",
    "Y = df_NCAA[['winner']]\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network using Keras\n",
    "Now that the data has been set up for the neural network, we will implement the neural network using Keras.\n",
    "\n",
    "For this neural network we define 14 nodes in the input layer (one node for each feature), 1 for the output layer (either 0 or 1 depending on whether a win or loss is predicted), and 12 neuron in the hidden layer. (From my reserach online, it is commonly believed that the number of hidden layers should fall somewhere between the input and output lairs. Based on that information, some trial and error was performed and it was discovered that 12 nodes worked well.\n",
    "\n",
    "I use the rectifier activation function (relu) on the first two layers and the sigmoid function in the output layer. The machine learning community used to prefer the sigmoid and tanh activation function for all layers, but through my research I discovered these days, better performance is achieved using the rectifier activation function. I use a sigmoid on the output layer to ensure our network output is between 0 and 1 and easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "6038/6038 [==============================] - 1s 218us/step - loss: 0.4789 - acc: 0.7559\n",
      "Epoch 2/150\n",
      "6038/6038 [==============================] - 1s 154us/step - loss: 0.1252 - acc: 0.9791\n",
      "Epoch 3/150\n",
      "6038/6038 [==============================] - 1s 153us/step - loss: 0.0684 - acc: 0.9856\n",
      "Epoch 4/150\n",
      "6038/6038 [==============================] - 1s 155us/step - loss: 0.0491 - acc: 0.9902\n",
      "Epoch 5/150\n",
      "6038/6038 [==============================] - 1s 160us/step - loss: 0.0387 - acc: 0.9924\n",
      "Epoch 6/150\n",
      "6038/6038 [==============================] - 1s 155us/step - loss: 0.0304 - acc: 0.9955\n",
      "Epoch 7/150\n",
      "6038/6038 [==============================] - 1s 155us/step - loss: 0.0260 - acc: 0.9970\n",
      "Epoch 8/150\n",
      "6038/6038 [==============================] - 1s 155us/step - loss: 0.0210 - acc: 0.9980\n",
      "Epoch 9/150\n",
      "6038/6038 [==============================] - 1s 153us/step - loss: 0.0180 - acc: 0.9983\n",
      "Epoch 10/150\n",
      "6038/6038 [==============================] - 1s 159us/step - loss: 0.0145 - acc: 0.9992\n",
      "Epoch 11/150\n",
      "6038/6038 [==============================] - 1s 183us/step - loss: 0.0130 - acc: 0.9987\n",
      "Epoch 12/150\n",
      "6038/6038 [==============================] - 1s 184us/step - loss: 0.0113 - acc: 0.9990\n",
      "Epoch 13/150\n",
      "6038/6038 [==============================] - 1s 164us/step - loss: 0.0103 - acc: 0.9988\n",
      "Epoch 14/150\n",
      "6038/6038 [==============================] - 1s 174us/step - loss: 0.0095 - acc: 0.9988\n",
      "Epoch 15/150\n",
      "6038/6038 [==============================] - 1s 157us/step - loss: 0.0080 - acc: 0.9997\n",
      "Epoch 16/150\n",
      "6038/6038 [==============================] - 1s 162us/step - loss: 0.0067 - acc: 0.9997\n",
      "Epoch 17/150\n",
      "6038/6038 [==============================] - 1s 196us/step - loss: 0.0072 - acc: 0.9997\n",
      "Epoch 18/150\n",
      "6038/6038 [==============================] - 1s 166us/step - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 19/150\n",
      "6038/6038 [==============================] - 1s 163us/step - loss: 0.0054 - acc: 0.9998\n",
      "Epoch 20/150\n",
      "6038/6038 [==============================] - 1s 164us/step - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 21/150\n",
      "6038/6038 [==============================] - 1s 167us/step - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 22/150\n",
      "6038/6038 [==============================] - 1s 186us/step - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 23/150\n",
      "6038/6038 [==============================] - 1s 168us/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 24/150\n",
      "6038/6038 [==============================] - 1s 159us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 25/150\n",
      "6038/6038 [==============================] - 1s 179us/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 26/150\n",
      "6038/6038 [==============================] - 1s 216us/step - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 27/150\n",
      "6038/6038 [==============================] - 1s 172us/step - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 28/150\n",
      "6038/6038 [==============================] - 1s 160us/step - loss: 0.0041 - acc: 0.9988\n",
      "Epoch 29/150\n",
      "6038/6038 [==============================] - 1s 194us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 30/150\n",
      "6038/6038 [==============================] - 1s 159us/step - loss: 0.0029 - acc: 0.9995\n",
      "Epoch 31/150\n",
      "6038/6038 [==============================] - 1s 161us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 32/150\n",
      "6038/6038 [==============================] - 1s 195us/step - loss: 0.0023 - acc: 0.9997\n",
      "Epoch 33/150\n",
      "6038/6038 [==============================] - 1s 158us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 34/150\n",
      "6038/6038 [==============================] - 1s 154us/step - loss: 0.0031 - acc: 0.9992\n",
      "Epoch 35/150\n",
      "6038/6038 [==============================] - 1s 216us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 36/150\n",
      "6038/6038 [==============================] - 1s 155us/step - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 37/150\n",
      "6038/6038 [==============================] - 1s 171us/step - loss: 8.5673e-04 - acc: 1.0000\n",
      "Epoch 38/150\n",
      "6038/6038 [==============================] - 1s 150us/step - loss: 8.2868e-04 - acc: 1.0000\n",
      "Epoch 39/150\n",
      "6038/6038 [==============================] - 1s 174us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 40/150\n",
      "6038/6038 [==============================] - 1s 159us/step - loss: 7.8260e-04 - acc: 1.0000\n",
      "Epoch 41/150\n",
      "6038/6038 [==============================] - 1s 157us/step - loss: 0.0010 - acc: 0.9998\n",
      "Epoch 42/150\n",
      "6038/6038 [==============================] - 1s 167us/step - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 43/150\n",
      "6038/6038 [==============================] - 1s 137us/step - loss: 5.8164e-04 - acc: 1.0000\n",
      "Epoch 44/150\n",
      "6038/6038 [==============================] - 1s 135us/step - loss: 7.5679e-04 - acc: 1.0000\n",
      "Epoch 45/150\n",
      "6038/6038 [==============================] - 1s 132us/step - loss: 7.9834e-04 - acc: 1.0000\n",
      "Epoch 46/150\n",
      "6038/6038 [==============================] - 1s 143us/step - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 47/150\n",
      "6038/6038 [==============================] - 1s 134us/step - loss: 4.3684e-04 - acc: 1.0000\n",
      "Epoch 48/150\n",
      "6038/6038 [==============================] - 1s 151us/step - loss: 4.9728e-04 - acc: 1.0000\n",
      "Epoch 49/150\n",
      "6038/6038 [==============================] - 1s 136us/step - loss: 3.9511e-04 - acc: 1.0000\n",
      "Epoch 50/150\n",
      "6038/6038 [==============================] - 1s 134us/step - loss: 0.0063 - acc: 0.9983\n",
      "Epoch 51/150\n",
      "6038/6038 [==============================] - 1s 133us/step - loss: 3.8014e-04 - acc: 1.0000\n",
      "Epoch 52/150\n",
      "6038/6038 [==============================] - 1s 159us/step - loss: 3.5402e-04 - acc: 1.0000\n",
      "Epoch 53/150\n",
      "6038/6038 [==============================] - 1s 136us/step - loss: 3.1247e-04 - acc: 1.0000\n",
      "Epoch 54/150\n",
      "6038/6038 [==============================] - 1s 148us/step - loss: 3.6339e-04 - acc: 1.0000\n",
      "Epoch 55/150\n",
      "6038/6038 [==============================] - 1s 147us/step - loss: 3.2433e-04 - acc: 1.0000\n",
      "Epoch 56/150\n",
      "6038/6038 [==============================] - 1s 134us/step - loss: 0.0040 - acc: 0.9993\n",
      "Epoch 57/150\n",
      "6038/6038 [==============================] - 1s 134us/step - loss: 3.3814e-04 - acc: 1.0000\n",
      "Epoch 58/150\n",
      "6038/6038 [==============================] - 1s 136us/step - loss: 2.7836e-04 - acc: 1.0000\n",
      "Epoch 59/150\n",
      "6038/6038 [==============================] - 1s 135us/step - loss: 2.6676e-04 - acc: 1.0000\n",
      "Epoch 60/150\n",
      "6038/6038 [==============================] - 1s 131us/step - loss: 3.0172e-04 - acc: 1.0000\n",
      "Epoch 61/150\n",
      "6038/6038 [==============================] - 1s 134us/step - loss: 0.0027 - acc: 0.9988\n",
      "Epoch 62/150\n",
      "6038/6038 [==============================] - 1s 135us/step - loss: 3.1841e-04 - acc: 1.0000\n",
      "Epoch 63/150\n",
      "6038/6038 [==============================] - 1s 134us/step - loss: 2.6318e-04 - acc: 1.0000\n",
      "Epoch 64/150\n",
      "6038/6038 [==============================] - 1s 135us/step - loss: 2.4254e-04 - acc: 1.0000\n",
      "Epoch 65/150\n",
      "6038/6038 [==============================] - 1s 137us/step - loss: 2.0680e-04 - acc: 1.0000\n",
      "Epoch 66/150\n",
      "6038/6038 [==============================] - 1s 147us/step - loss: 2.1825e-04 - acc: 1.0000\n",
      "Epoch 67/150\n",
      "6038/6038 [==============================] - 1s 136us/step - loss: 2.1324e-04 - acc: 1.0000\n",
      "Epoch 68/150\n",
      "6038/6038 [==============================] - 1s 126us/step - loss: 0.0064 - acc: 0.9988\n",
      "Epoch 69/150\n",
      "6038/6038 [==============================] - 1s 131us/step - loss: 1.9067e-04 - acc: 1.0000\n",
      "Epoch 70/150\n",
      "6038/6038 [==============================] - 1s 136us/step - loss: 1.8038e-04 - acc: 1.0000\n",
      "Epoch 71/150\n",
      "6038/6038 [==============================] - 1s 135us/step - loss: 1.6176e-04 - acc: 1.0000\n",
      "Epoch 72/150\n",
      "6038/6038 [==============================] - 1s 133us/step - loss: 1.6137e-04 - acc: 1.0000\n",
      "Epoch 73/150\n",
      "6038/6038 [==============================] - 1s 161us/step - loss: 1.5647e-04 - acc: 1.0000\n",
      "Epoch 74/150\n",
      "6038/6038 [==============================] - 1s 164us/step - loss: 1.6019e-04 - acc: 1.0000\n",
      "Epoch 75/150\n",
      "6038/6038 [==============================] - 1s 161us/step - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 76/150\n",
      "6038/6038 [==============================] - 1s 134us/step - loss: 2.7495e-04 - acc: 1.0000\n",
      "Epoch 77/150\n",
      "6038/6038 [==============================] - 1s 136us/step - loss: 1.3446e-04 - acc: 1.0000\n",
      "Epoch 78/150\n",
      "6038/6038 [==============================] - 1s 143us/step - loss: 1.3314e-04 - acc: 1.0000\n",
      "Epoch 79/150\n",
      "6038/6038 [==============================] - 1s 131us/step - loss: 1.2730e-04 - acc: 1.0000\n",
      "Epoch 80/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6038/6038 [==============================] - 1s 139us/step - loss: 1.2222e-04 - acc: 1.0000\n",
      "Epoch 81/150\n",
      "6038/6038 [==============================] - 1s 163us/step - loss: 1.1624e-04 - acc: 1.0000\n",
      "Epoch 82/150\n",
      "6038/6038 [==============================] - 1s 161us/step - loss: 1.4243e-04 - acc: 1.0000\n",
      "Epoch 83/150\n",
      "6038/6038 [==============================] - 2s 289us/step - loss: 0.0058 - acc: 0.9983\n",
      "Epoch 84/150\n",
      "6038/6038 [==============================] - 1s 189us/step - loss: 1.4768e-04 - acc: 1.0000\n",
      "Epoch 85/150\n",
      "6038/6038 [==============================] - 1s 176us/step - loss: 1.4664e-04 - acc: 1.0000\n",
      "Epoch 86/150\n",
      "6038/6038 [==============================] - 1s 182us/step - loss: 1.2781e-04 - acc: 1.0000\n",
      "Epoch 87/150\n",
      "6038/6038 [==============================] - 1s 171us/step - loss: 1.1222e-04 - acc: 1.0000\n",
      "Epoch 88/150\n",
      "6038/6038 [==============================] - 1s 178us/step - loss: 1.0253e-04 - acc: 1.0000\n",
      "Epoch 89/150\n",
      "6038/6038 [==============================] - 1s 181us/step - loss: 1.0766e-04 - acc: 1.0000\n",
      "Epoch 90/150\n",
      "6038/6038 [==============================] - 2s 274us/step - loss: 1.0136e-04 - acc: 1.0000 0s - loss: 8.9116e\n",
      "Epoch 91/150\n",
      "6038/6038 [==============================] - 1s 246us/step - loss: 9.7448e-05 - acc: 1.0000\n",
      "Epoch 92/150\n",
      "6038/6038 [==============================] - 1s 210us/step - loss: 1.7194e-04 - acc: 1.0000\n",
      "Epoch 93/150\n",
      "6038/6038 [==============================] - 1s 185us/step - loss: 0.0078 - acc: 0.9987\n",
      "Epoch 94/150\n",
      "6038/6038 [==============================] - 1s 196us/step - loss: 7.5961e-05 - acc: 1.0000\n",
      "Epoch 95/150\n",
      "6038/6038 [==============================] - 1s 242us/step - loss: 7.0728e-05 - acc: 1.0000\n",
      "Epoch 96/150\n",
      "6038/6038 [==============================] - 1s 214us/step - loss: 6.9264e-05 - acc: 1.0000\n",
      "Epoch 97/150\n",
      "6038/6038 [==============================] - 1s 204us/step - loss: 6.7318e-05 - acc: 1.0000\n",
      "Epoch 98/150\n",
      "6038/6038 [==============================] - 1s 178us/step - loss: 6.2826e-05 - acc: 1.0000\n",
      "Epoch 99/150\n",
      "6038/6038 [==============================] - 1s 137us/step - loss: 5.8635e-05 - acc: 1.0000\n",
      "Epoch 100/150\n",
      "6038/6038 [==============================] - 1s 137us/step - loss: 6.4120e-05 - acc: 1.0000\n",
      "Epoch 101/150\n",
      "6038/6038 [==============================] - 1s 135us/step - loss: 1.8760e-04 - acc: 1.0000\n",
      "Epoch 102/150\n",
      "6038/6038 [==============================] - 1s 166us/step - loss: 0.0044 - acc: 0.9992\n",
      "Epoch 103/150\n",
      "6038/6038 [==============================] - 1s 161us/step - loss: 8.2171e-05 - acc: 1.0000\n",
      "Epoch 104/150\n",
      "6038/6038 [==============================] - 1s 138us/step - loss: 8.4330e-05 - acc: 1.0000\n",
      "Epoch 105/150\n",
      "6038/6038 [==============================] - 1s 174us/step - loss: 6.7412e-05 - acc: 1.0000\n",
      "Epoch 106/150\n",
      "6038/6038 [==============================] - 1s 167us/step - loss: 6.6727e-05 - acc: 1.0000\n",
      "Epoch 107/150\n",
      "6038/6038 [==============================] - 1s 183us/step - loss: 6.5742e-05 - acc: 1.0000\n",
      "Epoch 108/150\n",
      "6038/6038 [==============================] - 1s 138us/step - loss: 5.9616e-05 - acc: 1.0000\n",
      "Epoch 109/150\n",
      "6038/6038 [==============================] - 1s 143us/step - loss: 6.7874e-05 - acc: 1.0000\n",
      "Epoch 110/150\n",
      "6038/6038 [==============================] - 1s 138us/step - loss: 5.2228e-05 - acc: 1.0000\n",
      "Epoch 111/150\n",
      "6038/6038 [==============================] - 1s 141us/step - loss: 5.1719e-05 - acc: 1.0000\n",
      "Epoch 112/150\n",
      "6038/6038 [==============================] - 1s 136us/step - loss: 9.3685e-04 - acc: 0.9997\n",
      "Epoch 113/150\n",
      "6038/6038 [==============================] - 1s 136us/step - loss: 0.0049 - acc: 0.9983\n",
      "Epoch 114/150\n",
      "6038/6038 [==============================] - 1s 136us/step - loss: 5.4997e-05 - acc: 1.0000\n",
      "Epoch 115/150\n",
      "6038/6038 [==============================] - 1s 139us/step - loss: 5.4252e-05 - acc: 1.0000\n",
      "Epoch 116/150\n",
      "6038/6038 [==============================] - 1s 137us/step - loss: 4.9293e-05 - acc: 1.0000\n",
      "Epoch 117/150\n",
      "6038/6038 [==============================] - 1s 136us/step - loss: 4.9142e-05 - acc: 1.0000\n",
      "Epoch 118/150\n",
      "6038/6038 [==============================] - 1s 138us/step - loss: 4.4870e-05 - acc: 1.0000\n",
      "Epoch 119/150\n",
      "6038/6038 [==============================] - 1s 137us/step - loss: 4.3276e-05 - acc: 1.0000\n",
      "Epoch 120/150\n",
      "6038/6038 [==============================] - 1s 161us/step - loss: 4.6086e-05 - acc: 1.0000\n",
      "Epoch 121/150\n",
      "6038/6038 [==============================] - 1s 164us/step - loss: 4.4358e-05 - acc: 1.0000\n",
      "Epoch 122/150\n",
      "6038/6038 [==============================] - 1s 137us/step - loss: 4.8591e-05 - acc: 1.0000\n",
      "Epoch 123/150\n",
      "6038/6038 [==============================] - 1s 137us/step - loss: 4.8339e-05 - acc: 1.0000\n",
      "Epoch 124/150\n",
      "6038/6038 [==============================] - 1s 135us/step - loss: 5.3865e-05 - acc: 1.0000\n",
      "Epoch 125/150\n",
      "6038/6038 [==============================] - 1s 137us/step - loss: 0.0064 - acc: 0.9988\n",
      "Epoch 126/150\n",
      "6038/6038 [==============================] - 1s 139us/step - loss: 4.3374e-05 - acc: 1.0000\n",
      "Epoch 127/150\n",
      "6038/6038 [==============================] - 1s 140us/step - loss: 3.5289e-05 - acc: 1.0000\n",
      "Epoch 128/150\n",
      "6038/6038 [==============================] - 1s 139us/step - loss: 3.1125e-05 - acc: 1.0000\n",
      "Epoch 129/150\n",
      "6038/6038 [==============================] - 1s 139us/step - loss: 2.9125e-05 - acc: 1.0000\n",
      "Epoch 130/150\n",
      "6038/6038 [==============================] - 1s 153us/step - loss: 2.9142e-05 - acc: 1.0000\n",
      "Epoch 131/150\n",
      "6038/6038 [==============================] - 1s 139us/step - loss: 2.9071e-05 - acc: 1.0000\n",
      "Epoch 132/150\n",
      "6038/6038 [==============================] - 1s 137us/step - loss: 2.8534e-05 - acc: 1.0000\n",
      "Epoch 133/150\n",
      "6038/6038 [==============================] - 1s 163us/step - loss: 3.0391e-05 - acc: 1.0000\n",
      "Epoch 134/150\n",
      "6038/6038 [==============================] - 1s 157us/step - loss: 2.6234e-05 - acc: 1.0000\n",
      "Epoch 135/150\n",
      "6038/6038 [==============================] - 1s 134us/step - loss: 0.0015 - acc: 0.9992\n",
      "Epoch 136/150\n",
      "6038/6038 [==============================] - 1s 136us/step - loss: 2.6644e-05 - acc: 1.0000\n",
      "Epoch 137/150\n",
      "6038/6038 [==============================] - 1s 134us/step - loss: 2.5210e-05 - acc: 1.0000\n",
      "Epoch 138/150\n",
      "6038/6038 [==============================] - 1s 137us/step - loss: 2.3653e-05 - acc: 1.0000\n",
      "Epoch 139/150\n",
      "6038/6038 [==============================] - 1s 137us/step - loss: 2.3019e-05 - acc: 1.0000\n",
      "Epoch 140/150\n",
      "6038/6038 [==============================] - 1s 137us/step - loss: 2.2578e-05 - acc: 1.0000\n",
      "Epoch 141/150\n",
      "6038/6038 [==============================] - 1s 157us/step - loss: 2.1576e-05 - acc: 1.0000\n",
      "Epoch 142/150\n",
      "6038/6038 [==============================] - 1s 135us/step - loss: 2.0790e-05 - acc: 1.0000\n",
      "Epoch 143/150\n",
      "6038/6038 [==============================] - 1s 128us/step - loss: 2.0929e-05 - acc: 1.0000\n",
      "Epoch 144/150\n",
      "6038/6038 [==============================] - 1s 136us/step - loss: 2.1731e-05 - acc: 1.0000\n",
      "Epoch 145/150\n",
      "6038/6038 [==============================] - 1s 169us/step - loss: 2.1041e-05 - acc: 1.0000\n",
      "Epoch 146/150\n",
      "6038/6038 [==============================] - 1s 142us/step - loss: 0.0133 - acc: 0.9972\n",
      "Epoch 147/150\n",
      "6038/6038 [==============================] - 1s 132us/step - loss: 4.0061e-05 - acc: 1.0000\n",
      "Epoch 148/150\n",
      "6038/6038 [==============================] - 1s 133us/step - loss: 3.5398e-05 - acc: 1.0000\n",
      "Epoch 149/150\n",
      "6038/6038 [==============================] - 1s 137us/step - loss: 3.4134e-05 - acc: 1.0000\n",
      "Epoch 150/150\n",
      "6038/6038 [==============================] - 1s 142us/step - loss: 3.1731e-05 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10d1a0a90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "np.random.seed(7)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X_scale, Y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6038/6038 [==============================] - 0s 47us/step\n",
      "\n",
      "acc: 86.07%\n",
      "6038/6038 [==============================] - 0s 30us/step\n",
      "\n",
      "acc: 86.07%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the dataset, the model predicts with an 86 percent accuracy. That being said, I didn't split the data into a train and test set. Once I finish this base prediction, I will redo the process also taking that into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X)\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see the predictions for all the games that were provided in the dataset. Now that I have learned how to implement the neural net, I will split the data into a train/test set which I can then use to predict then cross check with my current predictions provided by the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "4830/4830 [==============================] - 2s 337us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 2/150\n",
      "4830/4830 [==============================] - 1s 184us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 3/150\n",
      "4830/4830 [==============================] - 1s 192us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 4/150\n",
      "4830/4830 [==============================] - 1s 199us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 5/150\n",
      "4830/4830 [==============================] - 1s 285us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 6/150\n",
      "4830/4830 [==============================] - 1s 243us/step - loss: 10.7454 - acc: 0.33330s -\n",
      "Epoch 7/150\n",
      "4830/4830 [==============================] - 1s 254us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 8/150\n",
      "4830/4830 [==============================] - 1s 247us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 9/150\n",
      "4830/4830 [==============================] - 1s 245us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 10/150\n",
      "4830/4830 [==============================] - 1s 297us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 11/150\n",
      "4830/4830 [==============================] - 1s 253us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 12/150\n",
      "4830/4830 [==============================] - 1s 256us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 13/150\n",
      "4830/4830 [==============================] - 1s 178us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 14/150\n",
      "4830/4830 [==============================] - 1s 233us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 15/150\n",
      "4830/4830 [==============================] - 1s 241us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 16/150\n",
      "4830/4830 [==============================] - 1s 212us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 17/150\n",
      "4830/4830 [==============================] - 1s 220us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 18/150\n",
      "4830/4830 [==============================] - 1s 168us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 19/150\n",
      "4830/4830 [==============================] - 1s 177us/step - loss: 10.7454 - acc: 0.33330s - loss: 1\n",
      "Epoch 20/150\n",
      "4830/4830 [==============================] - 1s 177us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 21/150\n",
      "4830/4830 [==============================] - 1s 178us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 22/150\n",
      "4830/4830 [==============================] - 1s 185us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 23/150\n",
      "4830/4830 [==============================] - 1s 206us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 24/150\n",
      "4830/4830 [==============================] - 1s 220us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 25/150\n",
      "4830/4830 [==============================] - 1s 178us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 26/150\n",
      "4830/4830 [==============================] - 1s 189us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 27/150\n",
      "4830/4830 [==============================] - 1s 183us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 28/150\n",
      "4830/4830 [==============================] - 1s 188us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 29/150\n",
      "4830/4830 [==============================] - 1s 186us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 30/150\n",
      "4830/4830 [==============================] - 1s 227us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 31/150\n",
      "4830/4830 [==============================] - 1s 180us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 32/150\n",
      "4830/4830 [==============================] - 1s 174us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 33/150\n",
      "4830/4830 [==============================] - 1s 173us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 34/150\n",
      "4830/4830 [==============================] - 1s 207us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 35/150\n",
      "4830/4830 [==============================] - 1s 207us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 36/150\n",
      "4830/4830 [==============================] - 1s 214us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 37/150\n",
      "4830/4830 [==============================] - 1s 184us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 38/150\n",
      "4830/4830 [==============================] - 1s 189us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 39/150\n",
      "4830/4830 [==============================] - 1s 180us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 40/150\n",
      "4830/4830 [==============================] - 1s 181us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 41/150\n",
      "4830/4830 [==============================] - 1s 183us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 42/150\n",
      "4830/4830 [==============================] - 1s 185us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 43/150\n",
      "4830/4830 [==============================] - 1s 264us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 44/150\n",
      "4830/4830 [==============================] - 1s 263us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 45/150\n",
      "4830/4830 [==============================] - 1s 149us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 46/150\n",
      "4830/4830 [==============================] - 1s 147us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 47/150\n",
      "4830/4830 [==============================] - 1s 204us/step - loss: 10.7454 - acc: 0.33330s - loss: 10.59\n",
      "Epoch 48/150\n",
      "4830/4830 [==============================] - 1s 158us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 49/150\n",
      "4830/4830 [==============================] - 1s 174us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 50/150\n",
      "4830/4830 [==============================] - 1s 160us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 51/150\n",
      "4830/4830 [==============================] - 1s 145us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 52/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 53/150\n",
      "4830/4830 [==============================] - 1s 168us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 54/150\n",
      "4830/4830 [==============================] - 1s 172us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 55/150\n",
      "4830/4830 [==============================] - 1s 147us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 56/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 57/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 58/150\n",
      "4830/4830 [==============================] - 1s 143us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 59/150\n",
      "4830/4830 [==============================] - 1s 143us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 60/150\n",
      "4830/4830 [==============================] - 1s 145us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 61/150\n",
      "4830/4830 [==============================] - 1s 143us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 62/150\n",
      "4830/4830 [==============================] - 1s 143us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 63/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 64/150\n",
      "4830/4830 [==============================] - 1s 147us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 65/150\n",
      "4830/4830 [==============================] - 1s 146us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 66/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 67/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 68/150\n",
      "4830/4830 [==============================] - 1s 148us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 69/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 70/150\n",
      "4830/4830 [==============================] - 1s 204us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 71/150\n",
      "4830/4830 [==============================] - 1s 173us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 72/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 73/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 74/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 75/150\n",
      "4830/4830 [==============================] - 1s 146us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 76/150\n",
      "4830/4830 [==============================] - 1s 146us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 77/150\n",
      "4830/4830 [==============================] - 1s 145us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 78/150\n",
      "4830/4830 [==============================] - 1s 147us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 79/150\n",
      "4830/4830 [==============================] - 1s 141us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 80/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 81/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4830/4830 [==============================] - 1s 217us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 82/150\n",
      "4830/4830 [==============================] - 1s 175us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 83/150\n",
      "4830/4830 [==============================] - 1s 142us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 84/150\n",
      "4830/4830 [==============================] - 1s 198us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 85/150\n",
      "4830/4830 [==============================] - 1s 192us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 86/150\n",
      "4830/4830 [==============================] - 1s 171us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 87/150\n",
      "4830/4830 [==============================] - 1s 152us/step - loss: 10.7454 - acc: 0.33330s - loss: 10.7717 - acc: 0.33\n",
      "Epoch 88/150\n",
      "4830/4830 [==============================] - 1s 168us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 89/150\n",
      "4830/4830 [==============================] - 1s 200us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 90/150\n",
      "4830/4830 [==============================] - 1s 182us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 91/150\n",
      "4830/4830 [==============================] - 1s 210us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 92/150\n",
      "4830/4830 [==============================] - 1s 251us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 93/150\n",
      "4830/4830 [==============================] - 1s 199us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 94/150\n",
      "4830/4830 [==============================] - 1s 244us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 95/150\n",
      "4830/4830 [==============================] - 2s 351us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 96/150\n",
      "4830/4830 [==============================] - 1s 249us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 97/150\n",
      "4830/4830 [==============================] - 1s 264us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 98/150\n",
      "4830/4830 [==============================] - 1s 210us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 99/150\n",
      "4830/4830 [==============================] - 1s 271us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 100/150\n",
      "4830/4830 [==============================] - 1s 201us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 101/150\n",
      "4830/4830 [==============================] - 1s 256us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 102/150\n",
      "4830/4830 [==============================] - 1s 252us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 103/150\n",
      "4830/4830 [==============================] - 1s 203us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 104/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 105/150\n",
      "4830/4830 [==============================] - 1s 143us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 106/150\n",
      "4830/4830 [==============================] - 1s 224us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 107/150\n",
      "4830/4830 [==============================] - 1s 185us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 108/150\n",
      "4830/4830 [==============================] - 1s 184us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 109/150\n",
      "4830/4830 [==============================] - 1s 196us/step - loss: 10.7454 - acc: 0.33330s - loss: 10.7417 - acc: 0.\n",
      "Epoch 110/150\n",
      "4830/4830 [==============================] - 1s 176us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 111/150\n",
      "4830/4830 [==============================] - 1s 173us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 112/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 113/150\n",
      "4830/4830 [==============================] - 1s 170us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 114/150\n",
      "4830/4830 [==============================] - 1s 145us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 115/150\n",
      "4830/4830 [==============================] - 1s 155us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 116/150\n",
      "4830/4830 [==============================] - 1s 145us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 117/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 118/150\n",
      "4830/4830 [==============================] - 1s 169us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 119/150\n",
      "4830/4830 [==============================] - 1s 167us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 120/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 121/150\n",
      "4830/4830 [==============================] - 1s 143us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 122/150\n",
      "4830/4830 [==============================] - 1s 154us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 123/150\n",
      "4830/4830 [==============================] - 1s 150us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 124/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 125/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 126/150\n",
      "4830/4830 [==============================] - 1s 149us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 127/150\n",
      "4830/4830 [==============================] - 1s 167us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 128/150\n",
      "4830/4830 [==============================] - 1s 195us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 129/150\n",
      "4830/4830 [==============================] - 1s 210us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 130/150\n",
      "4830/4830 [==============================] - 1s 195us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 131/150\n",
      "4830/4830 [==============================] - 1s 177us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 132/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 133/150\n",
      "4830/4830 [==============================] - 1s 148us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 134/150\n",
      "4830/4830 [==============================] - 1s 143us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 135/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 136/150\n",
      "4830/4830 [==============================] - 1s 144us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 137/150\n",
      "4830/4830 [==============================] - 1s 143us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 138/150\n",
      "4830/4830 [==============================] - 1s 147us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 139/150\n",
      "4830/4830 [==============================] - 1s 167us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 140/150\n",
      "4830/4830 [==============================] - 1s 162us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 141/150\n",
      "4830/4830 [==============================] - 1s 187us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 142/150\n",
      "4830/4830 [==============================] - 1s 150us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 143/150\n",
      "4830/4830 [==============================] - 1s 184us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 144/150\n",
      "4830/4830 [==============================] - 1s 153us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 145/150\n",
      "4830/4830 [==============================] - 1s 168us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 146/150\n",
      "4830/4830 [==============================] - 1s 182us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 147/150\n",
      "4830/4830 [==============================] - 1s 146us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 148/150\n",
      "4830/4830 [==============================] - 1s 146us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 149/150\n",
      "4830/4830 [==============================] - 1s 145us/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 150/150\n",
      "4830/4830 [==============================] - 1s 145us/step - loss: 10.7454 - acc: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a24685b00>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# create model\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(12, input_dim=14, activation='relu'))\n",
    "model2.add(Dense(14, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model2.fit(X_train, y_train, epochs=150, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to predict with an accuracy of 33 percent. I do not think this is correct. Because of that I will go ahead and add teh validation_data as an input to the model.fit function to see if that will impact my total score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4830 samples, validate on 1208 samples\n",
      "Epoch 1/10\n",
      "4830/4830 [==============================] - 2s 321us/step - loss: 0.3412 - acc: 0.8907 - val_loss: 0.0893 - val_acc: 0.9677\n",
      "Epoch 2/10\n",
      "4830/4830 [==============================] - 1s 186us/step - loss: 0.0973 - acc: 0.9615 - val_loss: 0.0640 - val_acc: 0.9776\n",
      "Epoch 3/10\n",
      "4830/4830 [==============================] - 1s 186us/step - loss: 0.0647 - acc: 0.9776 - val_loss: 0.0381 - val_acc: 0.9868\n",
      "Epoch 4/10\n",
      "4830/4830 [==============================] - 1s 188us/step - loss: 0.0435 - acc: 0.9894 - val_loss: 0.0325 - val_acc: 0.9884\n",
      "Epoch 5/10\n",
      "4830/4830 [==============================] - 1s 219us/step - loss: 0.0374 - acc: 0.9907 - val_loss: 0.0346 - val_acc: 0.9909\n",
      "Epoch 6/10\n",
      "4830/4830 [==============================] - 1s 233us/step - loss: 0.0393 - acc: 0.9872 - val_loss: 0.0191 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "4830/4830 [==============================] - 1s 208us/step - loss: 0.0263 - acc: 0.9950 - val_loss: 0.0137 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "4830/4830 [==============================] - 1s 229us/step - loss: 0.0207 - acc: 0.9981 - val_loss: 0.0116 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "4830/4830 [==============================] - 1s 202us/step - loss: 0.0213 - acc: 0.9952 - val_loss: 0.0196 - val_acc: 0.9901\n",
      "Epoch 10/10\n",
      "4830/4830 [==============================] - 1s 245us/step - loss: 0.0401 - acc: 0.9855 - val_loss: 0.0116 - val_acc: 0.9992\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# create model\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(12, input_dim=14, activation='relu'))\n",
    "model1.add(Dense(14, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "history = model1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have changed my function so that it includes the validation set, my accuracy has hit 99 percent. This to me indicates I am doing something wrong or over fitting. I will visualize the accuracy with the epocs to better guage how the neural net has performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.996, Test: 0.999\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "# evaluate the modelx\n",
    "_, train_acc = model1.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc = model1.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# learning curves of model accuracy\n",
    "pyplot.plot(history.history['acc'], label='train')\n",
    "pyplot.plot(history.history['val_acc'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1208/1208 [==============================] - 0s 23us/step\n",
      "\n",
      "acc: 99.92%\n",
      "4830/4830 [==============================] - 0s 37us/step\n",
      "\n",
      "acc: 99.57%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model1.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "# evaluate the model\n",
    "scores = model1.evaluate(X_train, y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores do not seem to be correct. I am going to remake my neural network from scratch as I'm not too confident with what was done above. I will keep it in the notebook, however, as documentation for the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4226, 14) (906, 14) (906, 14) (4226, 1) (906, 1) (906, 1)\n",
      "Train on 4226 samples, validate on 906 samples\n",
      "Epoch 1/100\n",
      "4226/4226 [==============================] - 1s 213us/step - loss: 0.6483 - acc: 0.6668 - val_loss: 0.6527 - val_acc: 0.6369\n",
      "Epoch 2/100\n",
      "4226/4226 [==============================] - 0s 59us/step - loss: 0.6198 - acc: 0.6668 - val_loss: 0.6310 - val_acc: 0.6369\n",
      "Epoch 3/100\n",
      "4226/4226 [==============================] - 0s 56us/step - loss: 0.5970 - acc: 0.6668 - val_loss: 0.6081 - val_acc: 0.6369\n",
      "Epoch 4/100\n",
      "4226/4226 [==============================] - 0s 60us/step - loss: 0.5718 - acc: 0.6668 - val_loss: 0.5802 - val_acc: 0.6369\n",
      "Epoch 5/100\n",
      "4226/4226 [==============================] - 0s 63us/step - loss: 0.5411 - acc: 0.6675 - val_loss: 0.5467 - val_acc: 0.6744\n",
      "Epoch 6/100\n",
      "4226/4226 [==============================] - 0s 90us/step - loss: 0.5059 - acc: 0.7030 - val_loss: 0.5081 - val_acc: 0.7483\n",
      "Epoch 7/100\n",
      "4226/4226 [==============================] - 0s 58us/step - loss: 0.4655 - acc: 0.7709 - val_loss: 0.4670 - val_acc: 0.7616\n",
      "Epoch 8/100\n",
      "4226/4226 [==============================] - 0s 59us/step - loss: 0.4213 - acc: 0.8192 - val_loss: 0.4207 - val_acc: 0.8113\n",
      "Epoch 9/100\n",
      "4226/4226 [==============================] - 0s 59us/step - loss: 0.3762 - acc: 0.8656 - val_loss: 0.3775 - val_acc: 0.8389\n",
      "Epoch 10/100\n",
      "4226/4226 [==============================] - 0s 78us/step - loss: 0.3327 - acc: 0.9025 - val_loss: 0.3361 - val_acc: 0.8653\n",
      "Epoch 11/100\n",
      "4226/4226 [==============================] - 0s 79us/step - loss: 0.2928 - acc: 0.9295 - val_loss: 0.2900 - val_acc: 0.9294\n",
      "Epoch 12/100\n",
      "4226/4226 [==============================] - 0s 55us/step - loss: 0.2574 - acc: 0.9482 - val_loss: 0.2590 - val_acc: 0.9238\n",
      "Epoch 13/100\n",
      "4226/4226 [==============================] - 0s 62us/step - loss: 0.2273 - acc: 0.9531 - val_loss: 0.2273 - val_acc: 0.9536\n",
      "Epoch 14/100\n",
      "4226/4226 [==============================] - 0s 73us/step - loss: 0.2020 - acc: 0.9614 - val_loss: 0.2040 - val_acc: 0.9570\n",
      "Epoch 15/100\n",
      "4226/4226 [==============================] - 0s 82us/step - loss: 0.1813 - acc: 0.9688 - val_loss: 0.1838 - val_acc: 0.9658\n",
      "Epoch 16/100\n",
      "4226/4226 [==============================] - 0s 59us/step - loss: 0.1638 - acc: 0.9695 - val_loss: 0.1754 - val_acc: 0.9448\n",
      "Epoch 17/100\n",
      "4226/4226 [==============================] - 0s 56us/step - loss: 0.1502 - acc: 0.9711 - val_loss: 0.1572 - val_acc: 0.9592\n",
      "Epoch 18/100\n",
      "4226/4226 [==============================] - 0s 89us/step - loss: 0.1378 - acc: 0.9733 - val_loss: 0.1421 - val_acc: 0.9680\n",
      "Epoch 19/100\n",
      "4226/4226 [==============================] - 0s 68us/step - loss: 0.1278 - acc: 0.9735 - val_loss: 0.1394 - val_acc: 0.9581\n",
      "Epoch 20/100\n",
      "4226/4226 [==============================] - 0s 62us/step - loss: 0.1193 - acc: 0.9747 - val_loss: 0.1294 - val_acc: 0.9669\n",
      "Epoch 21/100\n",
      "4226/4226 [==============================] - 0s 60us/step - loss: 0.1119 - acc: 0.9759 - val_loss: 0.1178 - val_acc: 0.9724\n",
      "Epoch 22/100\n",
      "4226/4226 [==============================] - 0s 61us/step - loss: 0.1055 - acc: 0.9759 - val_loss: 0.1184 - val_acc: 0.9669\n",
      "Epoch 23/100\n",
      "4226/4226 [==============================] - 0s 94us/step - loss: 0.1001 - acc: 0.9782 - val_loss: 0.1069 - val_acc: 0.9735\n",
      "Epoch 24/100\n",
      "4226/4226 [==============================] - 0s 99us/step - loss: 0.0948 - acc: 0.9794 - val_loss: 0.1015 - val_acc: 0.9746\n",
      "Epoch 25/100\n",
      "4226/4226 [==============================] - 0s 61us/step - loss: 0.0905 - acc: 0.9796 - val_loss: 0.1010 - val_acc: 0.9735\n",
      "Epoch 26/100\n",
      "4226/4226 [==============================] - 0s 52us/step - loss: 0.0863 - acc: 0.9799 - val_loss: 0.0946 - val_acc: 0.9735\n",
      "Epoch 27/100\n",
      "4226/4226 [==============================] - 0s 60us/step - loss: 0.0827 - acc: 0.9799 - val_loss: 0.0900 - val_acc: 0.9790\n",
      "Epoch 28/100\n",
      "4226/4226 [==============================] - 0s 61us/step - loss: 0.0796 - acc: 0.9820 - val_loss: 0.1002 - val_acc: 0.9592\n",
      "Epoch 29/100\n",
      "4226/4226 [==============================] - 0s 55us/step - loss: 0.0766 - acc: 0.9815 - val_loss: 0.0868 - val_acc: 0.9768\n",
      "Epoch 30/100\n",
      "4226/4226 [==============================] - 0s 55us/step - loss: 0.0734 - acc: 0.9827 - val_loss: 0.0859 - val_acc: 0.9768\n",
      "Epoch 31/100\n",
      "4226/4226 [==============================] - 0s 77us/step - loss: 0.0711 - acc: 0.9839 - val_loss: 0.0762 - val_acc: 0.9823\n",
      "Epoch 32/100\n",
      "4226/4226 [==============================] - 0s 114us/step - loss: 0.0681 - acc: 0.9856 - val_loss: 0.0736 - val_acc: 0.9812\n",
      "Epoch 33/100\n",
      "4226/4226 [==============================] - 0s 102us/step - loss: 0.0661 - acc: 0.9867 - val_loss: 0.0743 - val_acc: 0.9801\n",
      "Epoch 34/100\n",
      "4226/4226 [==============================] - 0s 99us/step - loss: 0.0638 - acc: 0.9856 - val_loss: 0.0691 - val_acc: 0.9823\n",
      "Epoch 35/100\n",
      "4226/4226 [==============================] - 0s 82us/step - loss: 0.0618 - acc: 0.9870 - val_loss: 0.0713 - val_acc: 0.9790\n",
      "Epoch 36/100\n",
      "4226/4226 [==============================] - 0s 62us/step - loss: 0.0600 - acc: 0.9879 - val_loss: 0.0676 - val_acc: 0.9834\n",
      "Epoch 37/100\n",
      "4226/4226 [==============================] - 0s 55us/step - loss: 0.0582 - acc: 0.9886 - val_loss: 0.0632 - val_acc: 0.9857\n",
      "Epoch 38/100\n",
      "4226/4226 [==============================] - 0s 64us/step - loss: 0.0567 - acc: 0.9886 - val_loss: 0.0646 - val_acc: 0.9834\n",
      "Epoch 39/100\n",
      "4226/4226 [==============================] - 0s 83us/step - loss: 0.0552 - acc: 0.9896 - val_loss: 0.0636 - val_acc: 0.9823\n",
      "Epoch 40/100\n",
      "4226/4226 [==============================] - 0s 70us/step - loss: 0.0534 - acc: 0.9898 - val_loss: 0.1094 - val_acc: 0.9459\n",
      "Epoch 41/100\n",
      "4226/4226 [==============================] - 0s 75us/step - loss: 0.0530 - acc: 0.9894 - val_loss: 0.0779 - val_acc: 0.9636\n",
      "Epoch 42/100\n",
      "4226/4226 [==============================] - 0s 64us/step - loss: 0.0507 - acc: 0.9922 - val_loss: 0.0552 - val_acc: 0.9901\n",
      "Epoch 43/100\n",
      "4226/4226 [==============================] - 0s 73us/step - loss: 0.0491 - acc: 0.9912 - val_loss: 0.0730 - val_acc: 0.9713\n",
      "Epoch 44/100\n",
      "4226/4226 [==============================] - 0s 67us/step - loss: 0.0488 - acc: 0.9915 - val_loss: 0.0516 - val_acc: 0.9912\n",
      "Epoch 45/100\n",
      "4226/4226 [==============================] - 0s 66us/step - loss: 0.0462 - acc: 0.9917 - val_loss: 0.0514 - val_acc: 0.9901\n",
      "Epoch 46/100\n",
      "4226/4226 [==============================] - 0s 64us/step - loss: 0.0453 - acc: 0.9929 - val_loss: 0.0497 - val_acc: 0.9912\n",
      "Epoch 47/100\n",
      "4226/4226 [==============================] - 0s 64us/step - loss: 0.0445 - acc: 0.9929 - val_loss: 0.0485 - val_acc: 0.9912\n",
      "Epoch 48/100\n",
      "4226/4226 [==============================] - 0s 64us/step - loss: 0.0433 - acc: 0.9936 - val_loss: 0.0565 - val_acc: 0.9790\n",
      "Epoch 49/100\n",
      "4226/4226 [==============================] - 0s 53us/step - loss: 0.0421 - acc: 0.9941 - val_loss: 0.0675 - val_acc: 0.9691\n",
      "Epoch 50/100\n",
      "4226/4226 [==============================] - 0s 71us/step - loss: 0.0416 - acc: 0.9941 - val_loss: 0.0452 - val_acc: 0.9912\n",
      "Epoch 51/100\n",
      "4226/4226 [==============================] - 0s 58us/step - loss: 0.0402 - acc: 0.9953 - val_loss: 0.0448 - val_acc: 0.9923\n",
      "Epoch 52/100\n",
      "4226/4226 [==============================] - 0s 52us/step - loss: 0.0393 - acc: 0.9950 - val_loss: 0.0423 - val_acc: 0.9945\n",
      "Epoch 53/100\n",
      "4226/4226 [==============================] - 0s 52us/step - loss: 0.0386 - acc: 0.9931 - val_loss: 0.0427 - val_acc: 0.9923\n",
      "Epoch 54/100\n",
      "4226/4226 [==============================] - 0s 61us/step - loss: 0.0374 - acc: 0.9960 - val_loss: 0.0606 - val_acc: 0.9779\n",
      "Epoch 55/100\n",
      "4226/4226 [==============================] - 0s 59us/step - loss: 0.0376 - acc: 0.9938 - val_loss: 0.0433 - val_acc: 0.9912\n",
      "Epoch 56/100\n",
      "4226/4226 [==============================] - 0s 51us/step - loss: 0.0360 - acc: 0.9960 - val_loss: 0.0423 - val_acc: 0.9912\n",
      "Epoch 57/100\n",
      "4226/4226 [==============================] - 0s 56us/step - loss: 0.0354 - acc: 0.9965 - val_loss: 0.0385 - val_acc: 0.9956\n",
      "Epoch 58/100\n",
      "4226/4226 [==============================] - 0s 59us/step - loss: 0.0346 - acc: 0.9955 - val_loss: 0.0390 - val_acc: 0.9934\n",
      "Epoch 59/100\n",
      "4226/4226 [==============================] - 0s 63us/step - loss: 0.0339 - acc: 0.9953 - val_loss: 0.0365 - val_acc: 0.9956\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4226/4226 [==============================] - 0s 70us/step - loss: 0.0333 - acc: 0.9962 - val_loss: 0.0369 - val_acc: 0.9945\n",
      "Epoch 61/100\n",
      "4226/4226 [==============================] - 0s 55us/step - loss: 0.0325 - acc: 0.9969 - val_loss: 0.0350 - val_acc: 0.9967\n",
      "Epoch 62/100\n",
      "4226/4226 [==============================] - 0s 55us/step - loss: 0.0320 - acc: 0.9962 - val_loss: 0.0347 - val_acc: 0.9967\n",
      "Epoch 63/100\n",
      "4226/4226 [==============================] - 0s 54us/step - loss: 0.0316 - acc: 0.9957 - val_loss: 0.0526 - val_acc: 0.9779\n",
      "Epoch 64/100\n",
      "4226/4226 [==============================] - 0s 60us/step - loss: 0.0310 - acc: 0.9960 - val_loss: 0.0331 - val_acc: 0.9967\n",
      "Epoch 65/100\n",
      "4226/4226 [==============================] - 0s 52us/step - loss: 0.0304 - acc: 0.9969 - val_loss: 0.0448 - val_acc: 0.9834\n",
      "Epoch 66/100\n",
      "4226/4226 [==============================] - 0s 57us/step - loss: 0.0298 - acc: 0.9969 - val_loss: 0.0346 - val_acc: 0.9967\n",
      "Epoch 67/100\n",
      "4226/4226 [==============================] - 0s 58us/step - loss: 0.0293 - acc: 0.9967 - val_loss: 0.0314 - val_acc: 0.9978\n",
      "Epoch 68/100\n",
      "4226/4226 [==============================] - 0s 62us/step - loss: 0.0287 - acc: 0.9972 - val_loss: 0.0318 - val_acc: 0.9978\n",
      "Epoch 69/100\n",
      "4226/4226 [==============================] - 0s 70us/step - loss: 0.0282 - acc: 0.9967 - val_loss: 0.0316 - val_acc: 0.9978\n",
      "Epoch 70/100\n",
      "4226/4226 [==============================] - 0s 71us/step - loss: 0.0276 - acc: 0.9979 - val_loss: 0.0296 - val_acc: 0.9978\n",
      "Epoch 71/100\n",
      "4226/4226 [==============================] - 0s 91us/step - loss: 0.0272 - acc: 0.9974 - val_loss: 0.0291 - val_acc: 0.9978\n",
      "Epoch 72/100\n",
      "4226/4226 [==============================] - 1s 123us/step - loss: 0.0268 - acc: 0.9976 - val_loss: 0.0288 - val_acc: 0.9978\n",
      "Epoch 73/100\n",
      "4226/4226 [==============================] - 0s 44us/step - loss: 0.0265 - acc: 0.9979 - val_loss: 0.0289 - val_acc: 0.9978\n",
      "Epoch 74/100\n",
      "4226/4226 [==============================] - 0s 72us/step - loss: 0.0260 - acc: 0.9972 - val_loss: 0.0293 - val_acc: 0.9989\n",
      "Epoch 75/100\n",
      "4226/4226 [==============================] - 0s 43us/step - loss: 0.0257 - acc: 0.9981 - val_loss: 0.0271 - val_acc: 0.9978\n",
      "Epoch 76/100\n",
      "4226/4226 [==============================] - 0s 48us/step - loss: 0.0250 - acc: 0.9976 - val_loss: 0.0270 - val_acc: 0.9978\n",
      "Epoch 77/100\n",
      "4226/4226 [==============================] - 0s 74us/step - loss: 0.0246 - acc: 0.9981 - val_loss: 0.0278 - val_acc: 0.9989\n",
      "Epoch 78/100\n",
      "4226/4226 [==============================] - 0s 63us/step - loss: 0.0242 - acc: 0.9983 - val_loss: 0.0283 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "4226/4226 [==============================] - 0s 60us/step - loss: 0.0239 - acc: 0.9983 - val_loss: 0.0266 - val_acc: 0.9989\n",
      "Epoch 80/100\n",
      "4226/4226 [==============================] - 0s 54us/step - loss: 0.0237 - acc: 0.9983 - val_loss: 0.0340 - val_acc: 0.9934\n",
      "Epoch 81/100\n",
      "4226/4226 [==============================] - 0s 77us/step - loss: 0.0234 - acc: 0.9976 - val_loss: 0.0359 - val_acc: 0.9901\n",
      "Epoch 82/100\n",
      "4226/4226 [==============================] - 0s 56us/step - loss: 0.0232 - acc: 0.9983 - val_loss: 0.0243 - val_acc: 0.9978\n",
      "Epoch 83/100\n",
      "4226/4226 [==============================] - 0s 43us/step - loss: 0.0227 - acc: 0.9983 - val_loss: 0.0266 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "4226/4226 [==============================] - 0s 43us/step - loss: 0.0222 - acc: 0.9988 - val_loss: 0.0241 - val_acc: 0.9989\n",
      "Epoch 85/100\n",
      "4226/4226 [==============================] - 0s 47us/step - loss: 0.0220 - acc: 0.9986 - val_loss: 0.0239 - val_acc: 0.9989\n",
      "Epoch 86/100\n",
      "4226/4226 [==============================] - 0s 70us/step - loss: 0.0215 - acc: 0.9983 - val_loss: 0.0282 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "4226/4226 [==============================] - 0s 43us/step - loss: 0.0215 - acc: 0.9988 - val_loss: 0.0226 - val_acc: 0.9978\n",
      "Epoch 88/100\n",
      "4226/4226 [==============================] - 0s 42us/step - loss: 0.0210 - acc: 0.9988 - val_loss: 0.0222 - val_acc: 0.9978\n",
      "Epoch 89/100\n",
      "4226/4226 [==============================] - 0s 44us/step - loss: 0.0207 - acc: 0.9991 - val_loss: 0.0219 - val_acc: 0.9978\n",
      "Epoch 90/100\n",
      "4226/4226 [==============================] - 0s 43us/step - loss: 0.0205 - acc: 0.9991 - val_loss: 0.0216 - val_acc: 0.9978\n",
      "Epoch 91/100\n",
      "4226/4226 [==============================] - 0s 56us/step - loss: 0.0203 - acc: 0.9988 - val_loss: 0.0214 - val_acc: 0.9978\n",
      "Epoch 92/100\n",
      "4226/4226 [==============================] - 0s 58us/step - loss: 0.0200 - acc: 0.9986 - val_loss: 0.0217 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "4226/4226 [==============================] - 0s 60us/step - loss: 0.0198 - acc: 0.9988 - val_loss: 0.0234 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "4226/4226 [==============================] - 0s 73us/step - loss: 0.0196 - acc: 0.9986 - val_loss: 0.0216 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "4226/4226 [==============================] - 0s 70us/step - loss: 0.0192 - acc: 0.9991 - val_loss: 0.0224 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "4226/4226 [==============================] - 0s 66us/step - loss: 0.0189 - acc: 0.9991 - val_loss: 0.0204 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "4226/4226 [==============================] - 0s 43us/step - loss: 0.0188 - acc: 0.9988 - val_loss: 0.0219 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "4226/4226 [==============================] - 0s 44us/step - loss: 0.0184 - acc: 0.9991 - val_loss: 0.0196 - val_acc: 0.9978\n",
      "Epoch 99/100\n",
      "4226/4226 [==============================] - 0s 42us/step - loss: 0.0181 - acc: 0.9993 - val_loss: 0.0194 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "4226/4226 [==============================] - 0s 42us/step - loss: 0.0181 - acc: 0.9995 - val_loss: 0.0190 - val_acc: 0.9989\n"
     ]
    }
   ],
   "source": [
    "X = df_NCAA[['away_defensive_rating', 'away_field_goals' ,'away_free_throws', 'away_offensive_rating', 'away_points', 'away_three_point_field_goals', 'away_wins', 'home_defensive_rating', 'home_field_goals', 'home_free_throws', 'home_offensive_rating', 'home_points', 'home_three_point_field_goals', 'home_wins']]\n",
    "Y = df_NCAA[['winner']]\n",
    "\n",
    "#create X_scale to normalize values. This will help reduce overfitting in the neural network\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n",
    "\n",
    "#train test split\n",
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3)\n",
    "\n",
    "#second train test split to create validation sets and test sets\n",
    "#im doing this due to what appeared to be overfitting in the previous models\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "\n",
    "#get shape of different data splits\n",
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)\n",
    "\n",
    "#testing a new number of neurons for a second neural network. This time I am also including a second hidden layer to compare the results\n",
    "newModel = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(14,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "#this time I will try the sgd optimizer function instead of the Adam one used in the previous neural network\n",
    "#â€˜sgdâ€™ refers to stochastic gradient descent\n",
    "#The loss function for outputs that take the values 1 or 0 is called binary cross entropy.\n",
    "newModel.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#\n",
    "hist = newModel.fit(X_train, Y_train,\n",
    "          batch_size=32, epochs=100,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906/906 [==============================] - 0s 33us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9977924944812362"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newModel.evaluate(X_test, Y_test)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the second neural net that I have run also predicts with impecciable accuracy. The model has been trained and tested, but something is wrong as there is no way it could have predicted every game perfectly. I am going to test it on the NCAA bracket and see what results it gets, as I'm suspicious in its current form it will suffer from sever overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
